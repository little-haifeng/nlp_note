统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 $P(w_1,w_2,w_3...,w_m)$ 。其实就是想办法找到一个概率分布，它可以表示任意一个句子或序列出现的概率。目前在自然语言处理相关应用非常广泛，如语音识别, 机器翻译, 词性标注, 句法分析等。传统方法主要是基于统计学模型，最近几年基于神经网络的语言模型也越来越成熟。

要判断一段文字是不是一句自然语言，可以通过确定这段文字的概率分布来表示其存在的可能性。 语言模型中的词是有顺序的，给定m个词看这句话是不是一句合理的自然语言，关键是看这些词的排列顺序是不是正确的。所以统计语言模型的基本思想是计算条件概率。比如一段文字有${w_1,w_2,w_3...,w_m}$这m个词组成，看看是不是一句话，就可以用下面的公式计算其联合概率： 
$$
P(w_1,w_2,w_3...,w_m)=P(w_2|w_1)P(w_3|w_1,w_2)...P(w_m|w_1,w_2,...w_m-1)
$$
![](res/chapter6-1.png)
$$
\begin{aligned}
p(今天,是,春节,我们,都,休息)=&p(今天)*p(是|今天)*p(春节|今天,是)*p(我们|今天,是,春节)\\
&*p(都|今天,是,春节,我们)*p(休息|今天,是,春节,我们,都)
\end{aligned}
$$


在实际操作中，如果文本较长，$P(w_i|w_1,w_2,...w_i-1)$的估计会非常困难，所以就出现了一个简化模型N-gram语言模型：当前词只和它前面的n个词有关，与更前面的词无关，上面的式子就可以写成： 
$$
P(w_i|w_1,w_2,...w_i-1)=P(w_i|w_{i-(n-1)},...,w_{i-1})
$$
试想一下，如果不约束文本的长度，那后面的条件概率几乎是无法计算的，因为数量太多了，不仅有这个问题，还有如果 $w_1,w_2,w_3,...w_n$组成的序列不是一个常用的序列，在预料中找不到这样的词序组合，那对应序列算出来的概率会很小很小，但是你也不能不计算他，所以就会面临你要计算非常多的概率，但是计算出来的大部分值非常小，甚至接近于0，而这些接近于0的概率还不能舍弃，必须存在，这样就会造成数据稀疏的问题。

n-gram语言模型解决了用普通的条件概率计算句子概率参数太多难以训练的问题，理论上来说n取得越大保留的词序信息就越多，生成的句子越合理，但如果n取得比较大，同样会面临数据稀疏的问题，n-gram 模型解决了参数太多难以训练的问题，但没有解决数据稀疏的问题。

之所以存在数据稀疏的问题，是因为我们想把n取得大一点来更多的保留词序信息，但n太大会导导致$w_1,w_2,w_3,...w_n$这个序列在语料中很少出现或者根本不出现，（根据经验也知道太长的一句话在语料中出现的次数不是那么多，要远远小于2，3个词出现的次数）造成计算出的条件概率接近于0，那算出的句子的概率也是接近于0，这个语言模型就没法用了，这是数据稀疏导致的训练出的语言模型无法用的原因。

## Unigram model

**Unigram models** 也即一元文法模型，它是一种上下文无关模型。该模型仅仅考虑当前词本身出现的概率，而不考虑当前词的上下文环境。上下文之间相互独立，其概率形式为 
$$
P(w_1,w_2,w_3...,w_m)=P(w_1)*P(w_2)*P(w_3)*\cdots*P(w_m)
$$
即一个句子出现的概率等于句子中每个单词概率乘积。 以一篇文档为例，每个单词的概率只取决于该单词本身在文档中的概率，而文档中所有词出现的概率和为1，每个词的概率可以用该词在文档中出现的频率来表示.通常有些单词没有在语料库中出现。

## n-gram model

**ngram** 是一个最多包含n个元素的序列，这些元素从由它们们组成的序列(通常是字符串)中规取而成。一般来说， **n-grm** 的“元素” 可以是字符、音节、词，甚至是像 **ATGC** 等表示DNA序列的符号。

本节我们只关注词的 **n-grm** 而不关注 **ocr** 领域的字符的 **n-gram**， **n-gam** 不-定要求像复合词样有特定的含义，而仅仅要求出现频率足够高以引起词条计数器的注意。

为什么要使用 **n-gram** 呢?正如前面所看到的那样，当一个词条序列向量化成词袋向量时，它丢失了词序中所包含的很多含义。将单词条的概念扩展到多词条构成的 **n-gram** ，NLP流水线就可以保留语句词序中隐含的很多含义。例如，否定词 **“no”** 就会和它所属的相邻词在一起。如果分词不考虑n-gram, 那么 **“no”** 就会自由漂移，面不会固定在某几个词周围，其否定的含义可能就会与整个句子甚至整篇文档，而不是只与某几个相邻词关联。相比于词袋向量中的 **l-gam,2-gram** . **“was not”** 保留了两个独立词 **"not”** 和 **“was”** 的更多部分的含义。在流水线中如果把一个词和其相邻词捆绑起来，就会使词的一部分 上下文被保留。

针对参数空间过大的问题，Markov假设是解决该问题的有效方法，即随意一个词出现的概率只与它前面出现的一个或者几个词有关。一个句子${w_1,w_2,w_3...,w_m}$的概率表示如下：
$$
P(w_1,w_2,\cdots,w_m) = \prod_{i=0}^m P(w_i|w_1,\cdots,w_{i-1}) = \prod_{i=0}^m P(w_i|w_{i-(n-1)},\cdots,w_{i-1})
$$

```python
import re
from nltk.util import ngrams
from nltk.tokenize import TreebankWordTokenizer

# 正则表达式删除分c
sentence = "Thomas Jefferson began building Monticello at the age of 26"
pattern = re.compile(r"([-\s.,;!?])+")
tokens = pattern.split(sentence)
tokens = [x for x in tokens if x and x not in '- \t\n.,;!?']
print(list(ngrams(tokens,2)))
```



## 语言模型性能评价

如何衡量一一个语言模 型的好坏呢?一种方 法是将其应用于具体的外部任务(如机器翻译)，并根据该任务上指标的高低对语言模型进行评价。这种方法也被称为“外部任务评价”，是最接近实际应用需求的一种评价方法 。但是，这种方式的计算代价较高，实现的难度也较大。因此，目前最为常用的是基于困惑度 **( Perplexity,PPL)** 的“内部评价”方式。

为了进行内部评价，首先将数据划分为不相交的两个集合，分别称为训练集和测试集,其中训练集用于估计语言模型的参数。由该模型计算出的测试集的概率则反映了模型在测试集上的泛化能力”。

困惑度（perplexity）的基本思想是：**给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，**公式如下：
$$
perplexity = p(w_1,w_2,w_3,\cdots,w_m)^{-1/m}  \\
= \sqrt[m]{\prod_{i=1}^m \cfrac{1}{p(w_i|w_1,w_2,\cdots,w_{i-1})}}
$$
根号内是句子概率的倒数，所以显然 **句子越好（概率大），困惑度越小，也就是模型对句子越不困惑。** 公式中$i$的确定与我们采用 **n-gram** 模型有关，这样我们也就理解了这个指标的名字。简单来说，**perplexity** 刻画的是语言模型预测一个语言样本的能力，比如已经知道了 $(w_1,w_2,\cdots,w_m)$ 这句话会出现在语料库之中，那么通过语言模型计算得到这句话的概率越高，说明语言模型对这个语料库拟合的越好。

对于 **bigram** 模型而言：
$$
perplexity =  (\prod_{i=1}^mP(w_i|w_{i-1}))^{-\cfrac{1}{m}}
$$


在实际计算中，考虑到相比较乘积求平方根的方式，采用加法的形式可以加速计算，同时避免概率乘积数值过小而导致浮点数向下溢出的问题。
$$
PPL = 2^{-\frac{1}{m}\sum_{i=1}^m \log_2P(w_i|w_{i-1})}
$$


在数学上，**log perplexity** 可以看作真实分布与预测分布之间的 **交叉熵 Cross Entropy**, 交叉熵描述了两个概率分布之间的一种距离，假设 $x$ 是一个离散变量， 是 $u(x)\quad v(x)$ 两个与 $x$ 相关的概率分布，那么  之间$u \quad v$的交叉熵 的定义是分布 $u$下  的期望$-(\log v(x))$值：
$$
H(u,v)=E_u[-log v(x)] =-\sum_x u(x)\log v(x)
$$
我们把 $x$ 看作是单词， $u(x)$ 表示每个位置上单词的真实分布，
$$
u(x|w_1,w_2,\cdots,w_{i-1}) = 
\begin{cases} 
1 ,x=w_i \\ 0,x\neq w_i
\end{cases}
$$
$v(x)$ 是模型的预测分布 $p(w_i|w_1,w_2,\cdots,w_{i-1})$ ，那么即有


$$
\begin{aligned}
& H(u,v) = -\sum_x u(x)\log v(x)  \\
& =-\cfrac{1}{m}\sum_{i-1}^{m}(\sum_x u(x|w_1,\cdots,w_{i-1})p(w_i|w_1,w_2,\cdots w_{i-1}))  \\
& =-\cfrac{1}{m}\sum_{i-1}^{m}(1\times p(w_i|w_1,w_2,\cdots,w_{i-1})+\sum_{x\neq w_i}0\times p(w_i|w_1,w_2,\cdots,w_{i-1})  \\
& = -\cfrac{1}{m}\sum_{i-1}^{m}(1\times p(w_i|w_1,w_2,\cdots,w_{i-1})  \\
& = log(perplexity(S))
\end{aligned}
$$
因此 **log perplexity** 和交叉熵是等价的。

## Smooth 平滑

---

参考博客

1. [求通俗解释NLP里的perplexity是什么？](https://www.zhihu.com/question/58482430)

